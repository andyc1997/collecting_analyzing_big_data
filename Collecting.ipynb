{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='black'>Introduction</font>\n",
    "\n",
    "<font color='#404040'>The goal of this project is to review the comments of different universities in the U.K., and *[www.whatuni.com](https://www.whatuni.com/university-course-reviews/?pageno=1)* is a popular website where students give comments about their universities. The research project involves (1) collecting textual data from this website; (2) applying visualization and machine learning methods to review the comments; (3) giving insight for the results derived from the black box methods.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='black'> Data collection</font>\n",
    "\n",
    "<font color='#404040'>This jupyter notebook scrapes data from *[www.whatuni.com](https://www.whatuni.com/university-course-reviews/?pageno=1)* for 3 different universities, namely, *University of Oxford*, *University of Edinburgh* and *University of Warwick*. These universities are chosen based on my academic interests. Some explanations are added for each code block to illustrate why and how the codes are written.</font>\n",
    "\n",
    "<font color='#404040'>We use the following packages for data collection: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#404040'>Now, we specify the hyperlinks to the comments of the target universities: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base hyperlink\n",
    "hlink_basic = 'https://www.whatuni.com/university-course-reviews/{}/{}'\n",
    "\n",
    "# Hyperlink for different universities\n",
    "hlink_oxford = hlink_basic.format('university-of-oxford', '3757')\n",
    "hlink_edinburgh = hlink_basic.format('university-of-edinburgh', '5508')\n",
    "hlink_warwick = hlink_basic.format('university-of-warwick', '3771')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#404040'>Given there are category, question and reviews, we store these information into a dictionary, namely, *comment*. Since we repeat this action over a number of comments from different users. We wrap the codes into a function called *extract_comment*.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comment(rating_category, rating_question, rating_reviews, comment):\n",
    "    # Check if there is indeed a question\n",
    "    if rating_question is not None:\n",
    "        # Check if there is indeed a review\n",
    "        if rating_reviews is not None:\n",
    "            # If yes, append the contents\n",
    "            comment[rating_category.text] = {rating_question.text: rating_reviews.text}\n",
    "        \n",
    "        else:\n",
    "            # If no, rating_reviews is a None-type object\n",
    "            comment[rating_category.text] = {rating_question.text: rating_reviews}\n",
    "    \n",
    "    else:\n",
    "        # There is no question, return a None-type object\n",
    "        comment[rating_category.text] = None\n",
    "    \n",
    "    # Return the scraped comment\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#404040'>Given a hyperlink, we make a request to the target webpage and format the html page with *BeautifulSoup*. First, we should check if it is an empty page. Then, we proceed to find the html tags for the comments. Each comment begins with *&lt;div class:rlst_row&gt;*.</font>\n",
    "    \n",
    "<font color='#404040'>Inside the tag, we can find name, date, degree and review ratings by identifying the related html tags. Multiple categories are incorporated in review ratings. Now, we can use for-loop to go through all the categories, extract the questions and reviews by identifying the related html tags, and call *extract_comment*.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_summarize(hyperlink):\n",
    "    # Read and prettify the hyperlink\n",
    "    url_source = requests.get(hyperlink)\n",
    "    url_prettified = BeautifulSoup(url_source.content, 'html.parser', from_encoding = 'utf-8')\n",
    "\n",
    "    summary = []\n",
    "    \n",
    "    # Check whether the page has any comments, exit if no comments are found\n",
    "    if url_prettified.find('div', {'class':'rlst_row'}) is None:\n",
    "        return 'EMPTY'\n",
    "    \n",
    "    # Loop through each reply located in <div class=rlst_row> element\n",
    "    for case in url_prettified.find_all('div', {'class':'rlst_row'}):\n",
    "        # Craete an empty dictionary, corresponding to each observation\n",
    "        comment = dict()\n",
    "        \n",
    "        # Collect user information\n",
    "        name = case.find('div', {'class': 'rev_name'}).text\n",
    "        date = case.find('div', {'class':'rev_dte'}).text\n",
    "        degree = case.find('h3')\n",
    "        \n",
    "        # Some users do not give info about their degrees, it leads to 'degree' as a None object\n",
    "        if degree is not None:\n",
    "            degree = degree.text\n",
    "        \n",
    "        # Update\n",
    "        comment['name'] = name\n",
    "        comment['date'] = date\n",
    "        comment['degree'] = degree\n",
    "        \n",
    "        # Reviews are located in <div class=reviw_rating>\n",
    "        rating = case.find('div', {'class':'reviw_rating'})\n",
    "\n",
    "        # Each rating is located in <div class=rate_new>\n",
    "        cc = 1\n",
    "        prev_category = None\n",
    "        for subrating in rating.find_all('div', {'class':'rate_new'}):\n",
    "            # Each rating is associated with 3 parts: category, question, reviews\n",
    "            rating_category = subrating.find('span', {'class': 'cat_rat'})\n",
    "            rating_question = subrating.find('div', {'class': 'rw_qus_des'})\n",
    "            rating_reviews = subrating.find('p', {'class': 'rev_dec'})\n",
    "            \n",
    "            # Check if there is a category\n",
    "            if rating_category is not None:\n",
    "                # Update\n",
    "                comment = extract_comment(rating_category, rating_question, rating_reviews, comment)\n",
    "                prev_category = rating_category\n",
    "                cc = 1 # Reset counter\n",
    "            \n",
    "            # If there is no category, it means that the question and review belong the previous category\n",
    "            else:\n",
    "                # Update\n",
    "                comment = extract_comment(prev_category, rating_question, rating_reviews, comment)\n",
    "                cc += 1\n",
    "        \n",
    "        # Update\n",
    "        summary.append(comment)\n",
    "        \n",
    "    # Return the scraped comment\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#404040'>These selected universities are popular and famous, so there are multiple pages of comments about the universities. We need to format the hyperlink by adding *'?pageno={}'* before passing it to *comment_summarize* for scraping. We implement a for-loop to go over all possible pages until an empty page, and scrape all the textual data using the previous function.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(hyperlink):\n",
    "    # Access pages\n",
    "    pagelink = hyperlink + '?pageno={}'\n",
    "    total_summary = []\n",
    "    \n",
    "    # Loop through pages\n",
    "    for i in range(0, 200): \n",
    "        # Access i-th page, and scrape the comments by calling comment_summarize()\n",
    "        page_summary = comment_summarize(pagelink.format(i))\n",
    "\n",
    "        # In case i-th page is empty, break the loop because there are at most i - 1 pages\n",
    "        if page_summary == 'EMPTY':\n",
    "            print('There are at most {} pages'.format(i - 1))\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            # Update\n",
    "            total_summary.extend(page_summary)\n",
    "        \n",
    "        # Avoid visiting the website too quickly and get blocked\n",
    "        time.sleep(5)\n",
    "        \n",
    "    # Return the scraped comments\n",
    "    return total_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#404040'>With the predefined functions, we now can start the data collection process! Apply *get_content* to each university.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scrape websites\n",
    "content_oxford = get_content(hlink_oxford)\n",
    "content_edinburgh = get_content(hlink_edinburgh)\n",
    "content_warwick = get_content(hlink_warwick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='black'> Export data</font>\n",
    "\n",
    "<font color='#404040'>We convert the scraped data into a dataframe object in pandas, then export them to the data folder in the directory. Note that we are using *relative path* here. So, you do not need to change the path when running this notebook.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Export data\n",
    "def export_data(content_uni, name_uni):\n",
    "    pd.DataFrame(content_uni).to_csv('./data/' + name_uni + '.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_data(content_oxford, 'oxford')\n",
    "export_data(content_edinburgh, 'edinburgh')\n",
    "export_data(content_warwick, 'warwick')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#404040'>The data have been collected, and now we proceed to the data cleaning part in the next notebook.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
