{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39fe5cd8",
   "metadata": {},
   "source": [
    "### <font color='black'>Tagging and lemmatization</font>\n",
    "\n",
    "<font color='#404040'>In this notebook, we will perform POS tagging and lemmatization. In text mining, words are often having the same meaning or steming form the same origin. The results of analyzing the raw data can be unsatisfactory if we ignore such properties. Several possible questions are answered as follows:</font>\n",
    "\n",
    "---\n",
    "\n",
    "<font color='#404040'>**Question 1:** Why do we prefer lemmatization over stemming for this analysis?</font>\n",
    "\n",
    "<font color='#404040'>**Answer:** Both lemmatization and stemming are used to \"normalize\" the text. *Stemming* is a kind of heuristic which uses the *stem* of words (i.e. the substring of words), whereas lemmatization is taking the part-of-speech into account before normalization. So, lemmatization is more \"intelligent\" which is able to distinguish some confusing cases. That's why we also need to do POS tagging in this notebook.</font>\n",
    "\n",
    "---\n",
    "\n",
    "<font color='#404040'>**Question 2:** Why do we need to convert the data types from *string* to *numeric*?</font>\n",
    "\n",
    "<font color='#404040'>**Answer:** It is due to the statistical property of rating scores. We expect a higher score should imply a better rating, which is an ordinal ranking. However, *string* is not something ordinal in nature. From the statistical point of view, *numeric* is preferred over *string* for rating scores.</font>\n",
    "\n",
    "---\n",
    "\n",
    "<font color='#404040'>**Question 3:** Why do the data for different universities are merged together in the last section?</font>\n",
    "\n",
    "<font color='#404040'>**Answer:** For the convenience of file importing and exporting.</font>\n",
    "\n",
    "---\n",
    "\n",
    "<font color='#404040'>**Question 4:** Why do we need *reviews_lem* and *reviews_lem_short*?</font>\n",
    "\n",
    "<font color='#404040'>**Answer:** When using simple word tokenization (unigram), a clear topic pattern can be found using adjective and noun. Hence, we can experiment and compare the difference in topic modeling with *reviews_lem* and *reviews_lem_short*.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d3d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b78f2eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK Resource\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger') # for tagging\n",
    "nltk.download('stopwords') # stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fb98e0",
   "metadata": {},
   "source": [
    "### <font color='black'>Import data</font>\n",
    "\n",
    "<font color='#404040'>First, we import data cleaned in the previous notebook with relative paths.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95015813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "dat_oxford = pd.read_csv('./data/oxford_sum.csv')\n",
    "dat_edinburgh = pd.read_csv('./data/edinburgh_sum.csv')\n",
    "dat_warwick = pd.read_csv('./data/warwick_sum.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6923c94",
   "metadata": {},
   "source": [
    "### <font color='black'>Lemmatization</font>\n",
    "\n",
    "<font color='#404040'>As the lemmatization of WordNet is used, we need to convert *[NLTK part-of-speech tagging](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)* to *WordNet part-of-speech tagging*. So, we define *get_wordnet_pos* function which uses if-else statements to do this.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3821d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    # Adjective\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    \n",
    "    # Verb\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "\n",
    "    # Noun\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "    # Adverb\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    \n",
    "    # If not matched, return None-type object\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fea5a6c",
   "metadata": {},
   "source": [
    "<font color='#404040'>We also need to define a set of stopwords, which contains irrelevant words to be removed from the text. We use stopwords from the NLTK library.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb8732b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b0a876",
   "metadata": {},
   "source": [
    "<font color='#404040'>The following function performs POS tagging, lemmatization and stopwords filtering in a certain order:</font>\n",
    "\n",
    "<font color='#404040'>We apply POS tagging and then lemmatize the words (non-stop words) using the POS tags. Stopwords filtering comes after POS tagging because the removal of stopwords may impact on the results of POS tagging, for example, subjects and reflexive pronouns are removed. Lemmatization depends also on the part-of-speech; hence it should comes after POS tagging as well. Then, we apply the function *lemmatization* to the data.</font>\n",
    "\n",
    "<font color='#404040'>In addition, comments should be converted to lower case.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8320349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(comment):\n",
    "    # Tokenize the comment using NLTK and get their part-of-speeches\n",
    "    # Punctuations (e.g. comma and fullstop) are handled in the tokenization\n",
    "    words = []\n",
    "    words2 = []\n",
    "    tagging_nltk = nltk.pos_tag(word_tokenize(comment))\n",
    "    \n",
    "    # Convert the part-of-speeches into the WordNet format\n",
    "    # Store them in a list of tuples: (tokenized word in **lower case**, POS)\n",
    "    tagging_nltk = [*map(lambda s: (s[0].lower(), get_wordnet_pos(s[1])), tagging_nltk)]\n",
    "    \n",
    "    # Filter out the stopwords\n",
    "    tagging_nltk = [s for s in tagging_nltk if s[0] not in stop_words]\n",
    "        \n",
    "    # Loop through each word in the comment\n",
    "    for word, tag in tagging_nltk:\n",
    "        # If there is POS tagging, lemmatize it and add to a list\n",
    "        if tag is not None:\n",
    "            word_lem = WordNetLemmatizer().lemmatize(word, pos = tag)\n",
    "            words.append(word_lem)\n",
    "            \n",
    "            # Another list containing adjectives and nouns only\n",
    "            if tag not in (wordnet.VERB, wordnet.ADV):\n",
    "                words2.append(word_lem)\n",
    "                \n",
    "    # Join the lemmatized words and return a single string\n",
    "    return ' '.join(words), ' '.join(words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7343b5",
   "metadata": {},
   "source": [
    "<font color='#404040'>Below is a function which handles multiple output from *lemmatization*. It converts the tuple-like output into a pandas dataframe with 2 columns.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a62d20b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_output(pd_series):\n",
    "    # Apply lemmatization row-wise to dat_university['reviews']\n",
    "    # Use .tolist + pd.DataFrame() to convert multiple output into two columns \n",
    "    return pd.DataFrame(pd_series.apply(lambda x: lemmatization(x)).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c1edf5",
   "metadata": {},
   "source": [
    "<font color='#404040'>Lemmatize the dataset for each university. We use *apply* function because each entry in *reviews* column is a sentence.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ff15c59",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lemmatize the dataset\n",
    "dat_oxford[['reviews_lem', 'reviews_lem_short']] = multiple_output(dat_oxford['reviews'])\n",
    "dat_edinburgh[['reviews_lem', 'reviews_lem_short']] = multiple_output(dat_edinburgh['reviews'])\n",
    "dat_warwick[['reviews_lem', 'reviews_lem_short']] = multiple_output(dat_warwick['reviews'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769893e8",
   "metadata": {},
   "source": [
    "### <font color='black'>Data type</font>\n",
    "\n",
    "<font color='#404040'>The data type for rating scores are *string* but the scores actually range from 1 to 5. Hence, we need to convert these features to *numeric* data type because *string* does not show the ordinal ranking. Some examples with inconsistent / undesirable data types are listed as below:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec0c60c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rat4    541\n",
       "rat3    233\n",
       "rat5    229\n",
       "rat2     43\n",
       "rat1      8\n",
       "Name: score_Course and Lecturers, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the original data type\n",
    "# rat1 - rat5 (string) should be 1 to 5 (numeric)\n",
    "dat_edinburgh['score_Course and Lecturers'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81270811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratY    18\n",
       "ratN    18\n",
       "Name: score_Do you think your time at university this year has been value for money?, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the original data type\n",
    "# ratY and ratN (string) should be encoded to 1 and 0 (numeric)\n",
    "dat_edinburgh['score_Do you think your time at university this year has been value for money?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f51c9ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rat5       181\n",
       "rat4       107\n",
       "rat3        43\n",
       "rat2        13\n",
       "non_app     12\n",
       "rat1         4\n",
       "Name: score_Job Prospects, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the original data type\n",
    "# non_app should be replaced by NaN\n",
    "dat_oxford['score_Job Prospects'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d3c8b",
   "metadata": {},
   "source": [
    "<font color='#404040'>Below is a function to convert the appropriate data types. First, it checks whether the input is a missing value or *non_app* (standards for not applicable). For both cases, we return missing values.</font>\n",
    "\n",
    "<font color='#404040'>Otherwise, we extract the last character of the input because the raw scores are *rat1*,...,*rat5*, *ratY* and *ratN* which have the same pattern <font color='blue'>rat[score]</font>. If the scores are *Y* or *N*, we encode them into 1 and 0. Otherwise, they should be numeric from 1 to 5.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c59c31a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dtypes(x):\n",
    "    # Check if missing value\n",
    "    if pd.isnull(x) == True:\n",
    "        return x\n",
    "    \n",
    "    # If 'non_app', treat it as missing value\n",
    "    elif x == 'non_app':\n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        # Get the last character\n",
    "        x = x[-1]\n",
    "        \n",
    "        # If 'Y'/'N', return 1/0\n",
    "        if x == 'Y':\n",
    "            return 1\n",
    "        \n",
    "        elif x == 'N':\n",
    "            return 0\n",
    "        \n",
    "        # Otherwise, it ends with a number, convert it into float\n",
    "        else:\n",
    "            return float(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b6933",
   "metadata": {},
   "source": [
    "<font color='#404040'>Since such pattern can be found in every column starting with *score*, we need a *for-loop* to loop over the columns for each university.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79dcf61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_rating(dat_uni, uni_name):\n",
    "    # All columns with rating scores\n",
    "    score_columns = dat_uni.columns[dat_uni.columns.str.startswith('score_')]\n",
    "    \n",
    "    # Loop through each column and convert their data types\n",
    "    for column in score_columns:\n",
    "        dat_uni[column] = dat_uni[column].copy().apply(lambda x: convert_dtypes(x))\n",
    "    \n",
    "    return dat_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "872b23ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat_oxford = score_rating(dat_oxford, 'oxford')\n",
    "dat_edinburgh = score_rating(dat_edinburgh, 'edinburgh')\n",
    "dat_warwick = score_rating(dat_warwick, 'warwick')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac3a6d8",
   "metadata": {},
   "source": [
    "<font color='#404040'>We concatenate the data from different universities together using *pd.concat*. In the previous function *score_rating*, a column called *University* is created and can be used as an identifier.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77fd9ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate as the same dataframe\n",
    "def concat_dat_uni(dat_uni, dat_uni2, dat_uni3):\n",
    "    # Create a column to specify university\n",
    "    dat_uni['University'] = 'oxford'\n",
    "    dat_uni2['University'] = 'edinburgh'\n",
    "    dat_uni3['University'] = 'warwick'\n",
    "    \n",
    "    # Concatenate (row-wise, equivalently along axis = 0)\n",
    "    return pd.concat([dat_uni, dat_uni2, dat_uni3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f3fe649",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_uni = concat_dat_uni(dat_oxford, dat_edinburgh, dat_warwick)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16915d31",
   "metadata": {},
   "source": [
    "### <font color='black'>Export</font>\n",
    "\n",
    "<font color='#404040'>Our dataset is now ready for analysis, namely, *training_data.csv*. Please find topic modeling and text classification in the next notebook.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6734fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_uni.to_csv('./data/training_data.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
