{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6af5da36",
   "metadata": {},
   "source": [
    "### <font color='black'>Tagging and lemmatization</font>\n",
    "\n",
    "<font color='#404040'>In this notebook, we will perform POS tagging and lemmatization. In text mining, words are often having the same meaning or steming form the same origin. The results of analyzing the raw data can be unsatisfactory if we ignore such properties.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ff64977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1769f732",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK Resource\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger') # for tagging\n",
    "nltk.download('stopwords') # stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5280c8",
   "metadata": {},
   "source": [
    "### <font color='black'>Import data</font>\n",
    "\n",
    "<font color='#404040'>First, we import data cleaned in the previous notebook with relative paths.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb51522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "dat_oxford = pd.read_csv('./data/oxford_sum.csv')\n",
    "dat_edinburgh = pd.read_csv('./data/edinburgh_sum.csv')\n",
    "dat_warwick = pd.read_csv('./data/warwick_sum.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3647b70f",
   "metadata": {},
   "source": [
    "### <font color='black'>Lemmatization</font>\n",
    "\n",
    "<font color='#404040'>As the lemmatization of WordNet is used, we need to convert *[NLTK part-of-speech tagging](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)* to *WordNet part-of-speech tagging*. So, we define *get_wordnet_pos* function which uses if-else statements to do this.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d38d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    # Adjective\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    \n",
    "    # Verb\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "\n",
    "    # Noun\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "    # Adverb\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    \n",
    "    # If not matched, return None-type object\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48122ad1",
   "metadata": {},
   "source": [
    "<font color='#404040'>We also need to define a set of stopwords, which contains irrelevant words to be removed from the text. We use stopwords from the NLTK library.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7e47d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02719ff2",
   "metadata": {},
   "source": [
    "<font color='#404040'>The following function performs POS tagging, lemmatization and stopwords filtering. We apply POS tagging and then lemmatize the words (non-stop words) using the POS tags. Stopwords filtering comes after POS tagging because the removal of stopwords may impact on the results of POS tagging. Lemmatization depends also on the part-of-speech; hence it should comes after POS tagging as well. Then, we apply the function *lemmatization* to the data.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c699a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(comment):\n",
    "    # Tokenize the comment using NLTK and get their part-of-speeches\n",
    "    # Punctuations (e.g. comma and fullstop) are handled in the tokenization\n",
    "    words = []\n",
    "    tagging_nltk = nltk.pos_tag(word_tokenize(comment))\n",
    "    \n",
    "    # Convert the part-of-speeches into the WordNet format\n",
    "    # Store them in a list of tuples: (tokenized word, POS)\n",
    "    tagging_nltk = [*map(lambda s: (s[0], get_wordnet_pos(s[1])), tagging_nltk)]\n",
    "    \n",
    "    # Filter out the stopwords\n",
    "    tagging_nltk = [s for s in tagging_nltk if s[0] not in stop_words]\n",
    "        \n",
    "    # Loop through each word in the comment\n",
    "    for word, tag in tagging_nltk:\n",
    "        # If there is POS tagging, lemmatize it and add to a list\n",
    "        if tag is not None:\n",
    "            words.append(WordNetLemmatizer().lemmatize(word, pos = tag))\n",
    "    \n",
    "    # Join the lemmatized words and return a single string\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ca7667f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lemmatize the dataset\n",
    "dat_oxford['reviews_lem'] = dat_oxford['reviews'].apply(lambda x: lemmatization(x))\n",
    "dat_edinburgh['reviews_lem'] = dat_edinburgh['reviews'].apply(lambda x: lemmatization(x))\n",
    "dat_warwick['reviews_lem'] = dat_warwick['reviews'].apply(lambda x: lemmatization(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1171a1",
   "metadata": {},
   "source": [
    "### <font color='black'>Data type</font>\n",
    "\n",
    "<font color='#404040'>The data type for rating scores are *string* but the scores actually range from 1 to 5. Hence, we need to convert these features to *numeric* data type. Some examples with inconsistent / undesirable data types are listed as below:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77e0fcc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rat4    541\n",
       "rat3    233\n",
       "rat5    229\n",
       "rat2     43\n",
       "rat1      8\n",
       "Name: score_Course and Lecturers, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the original data type\n",
    "# rat1 - rat5 (string) should be 1 to 5 (numeric)\n",
    "dat_edinburgh['score_Course and Lecturers'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cacb3eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratY    18\n",
       "ratN    18\n",
       "Name: score_Do you think your time at university this year has been value for money?, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the original data type\n",
    "# ratY and ratN (string) should be encoded to 1 and 0 (numeric)\n",
    "dat_edinburgh['score_Do you think your time at university this year has been value for money?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b84b255",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rat5       181\n",
       "rat4       107\n",
       "rat3        43\n",
       "rat2        13\n",
       "non_app     12\n",
       "rat1         4\n",
       "Name: score_Job Prospects, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the original data type\n",
    "# non_app should be replaced by NaN\n",
    "dat_oxford['score_Job Prospects'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5998cf58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb7097ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dtypes(x):\n",
    "    # Check if missing value\n",
    "    if pd.isnull(x) == True:\n",
    "        return x\n",
    "    \n",
    "    # If 'non_app', treat it as missing value\n",
    "    elif x == 'non_app':\n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        # Get the last character\n",
    "        x = x[-1]\n",
    "        \n",
    "        # If 'Y'/'N', return 1/0\n",
    "        if x == 'Y':\n",
    "            return 1\n",
    "        \n",
    "        elif x == 'N':\n",
    "            return 0\n",
    "        \n",
    "        # Otherwise, it ends with a number, convert it into float\n",
    "        else:\n",
    "            return float(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07f5360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_rating(dat_uni):\n",
    "    score_columns = dat_uni.columns[dat_uni.columns.str.startswith('score_')]\n",
    "    for column in score_columns:\n",
    "        dat_uni[column] = dat_uni[column].copy().apply(lambda x: convert_dtypes(x))\n",
    "    return dat_uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dab4130",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dat_oxford = score_rating(dat_oxford)\n",
    "dat_edinburgh = score_rating(dat_edinburgh)\n",
    "dat_warwick = score_rating(dat_warwick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db3e156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
